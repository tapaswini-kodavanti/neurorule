{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d98b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab407b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb04f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0446d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SYNTHESIS FUNCTIONS ###\n",
    "\n",
    "def synthesize_data(X_df, numeric_cols, categorical_cols,\n",
    "                            n_samples=5000, gamma=0.1, low_pct=1.0, high_pct=99.0,\n",
    "                            random_state=None):\n",
    "    \n",
    "    def compute_bounds(X_df, numeric_cols, low_pct=1.0, high_pct=99.0, gamma=0.1):\n",
    "        # X_df: pandas DataFrame of training data (raw/unscaled)\n",
    "        mins = X_df[numeric_cols].quantile(low_pct/100.0).values\n",
    "        maxs = X_df[numeric_cols].quantile(high_pct/100.0).values\n",
    "        spans = maxs - mins\n",
    "        # avoid zero spans\n",
    "        spans[spans == 0] = 1e-6\n",
    "        low = mins - gamma * spans\n",
    "        high = maxs + gamma * spans\n",
    "        return low, high\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    low, high = compute_bounds(X_df, numeric_cols, low_pct=low_pct, high_pct=high_pct, gamma=gamma)\n",
    "    n_num = len(numeric_cols)\n",
    "    Xs_num = rng.uniform(low=low, high=high, size=(n_samples, n_num))\n",
    "\n",
    "    # categorical: sample by empirical frequencies\n",
    "    Xs_cat = {}\n",
    "    for c in categorical_cols:\n",
    "        vals, counts = np.unique(X_df[c].values, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        picks = rng.choice(len(vals), size=n_samples, p=probs)\n",
    "        Xs_cat[c] = vals[picks]\n",
    "\n",
    "    # assemble DataFrame\n",
    "    df_num = pd.DataFrame(Xs_num, columns=numeric_cols)\n",
    "    df_cat = pd.DataFrame(Xs_cat)\n",
    "    Xs = pd.concat([df_num, df_cat.reset_index(drop=True)], axis=1)[list(numeric_cols) + list(categorical_cols)]\n",
    "    return Xs\n",
    "\n",
    "\n",
    "# Generate synthetic data according to established distribution\n",
    "def synthesize_to_distribution(X_df, model, scaler, target_dist, conf_threshold=0.8, n_total=1000):\n",
    "    model.eval()\n",
    "\n",
    "    X_scale = scaler.fit_transform(X_df)\n",
    "    X_tensor = torch.tensor(X_scale, dtype=torch.float32)\n",
    "\n",
    "    # Run through model\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        confs, preds = torch.max(probs, dim=1)\n",
    "\n",
    "    probs_np = probs.numpy()\n",
    "    confs_np = confs.numpy()\n",
    "    preds_np = preds.numpy()\n",
    "\n",
    "    # Filter by confidence\n",
    "    conf_mask = confs_np >= conf_threshold\n",
    "    X_conf = X_df[conf_mask]\n",
    "    probs_conf = probs_np[conf_mask]\n",
    "    confs_conf = confs_np[conf_mask]\n",
    "    preds_conf = preds_np[conf_mask]\n",
    "\n",
    "    class_counts = {\n",
    "        c: int(round(frac * n_total)) for c, frac in target_dist.items()\n",
    "    }\n",
    "\n",
    "    # Select top confident samples\n",
    "    selected_idx = []\n",
    "    for c, count in class_counts.items():\n",
    "        idx_c = np.where(preds_conf == c)[0]\n",
    "        if len(idx_c) == 0:\n",
    "            print(\"something went wrong; no samples are above confidence threshold for class:\", c)\n",
    "\n",
    "        order = np.argsort(-confs_conf[idx_c])\n",
    "        top_idx = idx_c[order[:count]]\n",
    "        selected_idx.extend(top_idx)\n",
    "    \n",
    "    # Filter out records\n",
    "    X_selected = pd.DataFrame(np.array(X_conf)[selected_idx])\n",
    "    X_selected.columns = X_df.columns\n",
    "    y_selected = preds_conf[selected_idx]\n",
    "    probs_selected = probs_conf[selected_idx]\n",
    "\n",
    "    return X_selected, y_selected, probs_selected\n",
    "\n",
    "def output_dataset(X, y, X_synth, y_synth, class_names, use_baseline, filename):\n",
    "    X_synth = pd.DataFrame(X_synth, columns=X.columns)\n",
    "    y_synth = pd.get_dummies(y_synth)\n",
    "    y_synth.columns = class_names\n",
    "\n",
    "    # Force boolean columns in synthetic data to bool\n",
    "    bool_cols = X.select_dtypes(include='bool').columns\n",
    "    for col in bool_cols:\n",
    "        X_synth[col] = X_synth[col].astype(int)\n",
    "        X[col] = X[col].astype(int)\n",
    "\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "    synth_data = pd.concat([X_synth, y_synth], axis=1)\n",
    "\n",
    "    if use_baseline:\n",
    "        final_data = pd.concat([data, synth_data], ignore_index=True)\n",
    "    else:\n",
    "        final_data = synth_data\n",
    "    \n",
    "    final_data.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838fb9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.33%\n"
     ]
    }
   ],
   "source": [
    "# Synthetic data generator: It loads data and generates synthetic data within the given distribution\n",
    "# We shouldn't be doing extrapolated data generation\n",
    "\n",
    "##### 1. LOAD DATA AND MODEL WEIGHTS #####\n",
    "\n",
    "data_filename = \"../data/kidney/datasets/baseline/99.8_percent/in_distribution.csv\"\n",
    "weights_filename = \"../data/kidney/model_weights/baseline/99.8_percent.pth\"\n",
    "uci_function = None\n",
    "\n",
    "# class_names = [\"malignant\", \"benign\"]\n",
    "# class_names = ['class_0', 'class_1', 'class_2']\n",
    "# class_names = [\"HeartDisease_0\", \"HeartDisease_1\"]\n",
    "class_names = [\"CKD_Status_0\", \"CKD_Status_1\"]\n",
    "\n",
    "# Option 1: UCI Dataset\n",
    "if uci_function:\n",
    "    dataset = uci_function()\n",
    "    class_names = dataset.target_names\n",
    "    X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "    y = pd.DataFrame(dataset.target, columns=['target'])\n",
    "\n",
    "# Option 2: Load data from file\n",
    "else:\n",
    "    # may be expecting one-shot data?????\n",
    "    dataset = pd.read_csv(data_filename)\n",
    "    X = dataset.drop(columns=class_names)\n",
    "    y = dataset[class_names]\n",
    "\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "num_classes = y.shape[1]\n",
    "\n",
    "X_tensor = torch.tensor(StandardScaler().fit_transform(X), dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values.squeeze(), dtype=torch.long).argmax(dim=1) # not one-hot encoded\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "\n",
    "if weights_filename:\n",
    "    model = MLP(input_dim, num_classes)\n",
    "    model.load_state_dict(torch.load(weights_filename))\n",
    "    model.eval()\n",
    "    predictions = model(X_tensor)\n",
    "\n",
    "    _, predicted_classes = torch.max(predictions, 1)\n",
    "    accuracy = (predicted_classes == y_tensor).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy.item()*100:.2f}%\")\n",
    "else:\n",
    "    print(\"model weights not loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a4b1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.8001030683517456, 1: 0.19989696145057678}\n"
     ]
    }
   ],
   "source": [
    "##### 2. CALCULATE TARGET DISTRIBUTION #####\n",
    "\n",
    "counts = torch.bincount(y_tensor, minlength=num_classes)\n",
    "dist = counts.float() / counts.sum()\n",
    "distribution_map = {i: dist[i].item() for i in range(num_classes)}\n",
    "\n",
    "print(distribution_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d57ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means difference:\n",
      " Creatinine                   -0.715767\n",
      "BUN                         -14.779775\n",
      "GFR                          10.765822\n",
      "Urine_Output               -199.982905\n",
      "Diabetes                     -0.378305\n",
      "Hypertension                  0.002968\n",
      "Age                         -16.244711\n",
      "Protein_in_Urine           -765.333002\n",
      "Water_Intake                 -0.286709\n",
      "Medication_ACE Inhibitor     -0.286034\n",
      "Medication_ARB               -0.126019\n",
      "Medication_Diuretic          -0.512768\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "##### 3. SYNTHESIZE DATA #####\n",
    "\n",
    "num_samples = 300\n",
    "multiplier = 1000 # just generates extra data so that more data falls above the confidence threshold\n",
    "conf_threshold = 0.7\n",
    "filename = \"../data/kidney/datasets/mix/99.8_percent/300_samples.csv\"\n",
    "\n",
    "use_baseline = True\n",
    "\n",
    "\n",
    "X_synth_raw = synthesize_data(X, numeric_cols, categorical_cols, num_samples * multiplier, gamma=0.0)\n",
    "X_synth, y_synth, preds = synthesize_to_distribution(X_synth_raw, model, StandardScaler(), distribution_map, conf_threshold, num_samples)\n",
    "output_dataset(X, y, X_synth, y_synth, class_names, use_baseline, filename)\n",
    "\n",
    "means_original = X.mean()\n",
    "means_synthetic = X_synth.mean()\n",
    "\n",
    "# print(\"Means of original data:\\n\", means_original)\n",
    "# print(\"Means of synthetic data:\\n\", means_synthetic)\n",
    "\n",
    "diff = means_original - means_synthetic\n",
    "print(\"Means difference:\\n\", diff)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurorule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
